#!/usr/bin/env python3
'''HDF5pp_repair
  Extract readable data from a HDF5-file and copy it to a new HDF5-file.

Usage:
  HDF5pp_repair [options] <source> <destination>

Arguments:
  <source>        Source HDF5-file, possibly containing corrupted data.
  <destination>   Destination HDF5-file.

Options:
  -f, --force     Force continuation, overwrite existing files.
  -h, --help      Show help.
      --version   Show version.

(c - MIT) T.W.J. de Geus | tom@geus.me | www.geus.me | github.com/tdegeus/HDF5pp
'''

# ==================================================================================================

# temporary fix: suppress warning from h5py
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import sys, os, re, h5py, docopt

# ==================================================================================================

def confirm(message='Proceed [y/n]?\n'):
  r'''
Prompt user for confirmation. The function loops until the user responds with

* 'y' -> True
* 'n' -> False
  '''

  while True:

    # - prompt message, get user's response
    user = input(message)

    # - check response
    if not user                     : print('Please enter y or n.'); continue
    if user not in ['y','Y','n','N']: print('Please enter y or n.'); continue
    if user     in ['y','Y'        ]: return True
    if user     in ['n','N'        ]: return False

# ==================================================================================================

def error(message):
  r'''
Print error message and quit.
  '''

  print(message)

  sys.exit(1)

# ==================================================================================================

def quit(message):
  r'''
Prompt user for confirmation. If the response is negative this function quits the program.
  '''

  if not confirm(message):
    sys.exit(1)

# ==================================================================================================

def abspath(path):
  r'''
Return absolute path.
  '''

  return os.path.normpath(os.path.join('/', path))

# ==================================================================================================

def getdatasets(data, root="/"):
  r'''
Get a list of paths to each dataset in the HDF5-archive.
  '''

  # convert to absolute path
  root = abspath(root)

  # empty list of paths
  datasets = []

  # loop over all fields under the current root
  for name in data[root]:

    # - convert to path
    path = os.path.join(root, name)

    # - current path is a Dataset -> append list, otherwise append list with list of paths
    if isinstance(data[path], h5py.Dataset): datasets += [path]
    else                                   : datasets += getdatasets(data,path)

  # return list of paths
  return datasets

# ==================================================================================================

def verify(data, datasets):
  r'''
Try reading each dataset of a list of datasets. Return a list with only those datasets that can be
successfully opened.
  '''

  # empty list of paths
  out = []

  # loop over list of paths
  for path in datasets:

    # - try reading, move to the next path if reading is unsuccessful
    try   : data[path]
    except: continue

    # - add to output
    out += [path]

  # return list of paths that can be successfully read
  return out

# ==================================================================================================

def exists(data, datasets):
  r'''
Return the first path that exists in the HDF5-archive.
  '''

  for path in datasets:
    if path in data:
      return path

  return False

# ==================================================================================================

def copydatasets(source, dest, source_datasets, dest_datasets=None):
  r'''
Copy all datasets from one HDF5-archive 'source' to another HDF5-archive 'dest'. The datasets
can be renamed by specifying a list of 'dest_datasets' (whose entries should correspond to the
'source_datasets').
  '''

  # make sure that all paths are absolute paths
  source_datasets = [abspath(path) for path in source_datasets]

  # copy the source datasets
  if not dest_datasets: dest_datasets = [path for path in source_datasets]

  # get the group-names from 'source_datasets'
  # - read and filter duplicates
  groups = list(set([os.path.split(path)[0] for path in dest_datasets]))
  # - remove '/'
  groups = [group for group in groups if group != '/']
  # - sort based on depth
  groups = sorted(groups, key=lambda group: (group.count('/'), group))

  # create groups
  for group in groups: dest.create_group(group)

  # copy datasets
  for source_path, dest_path in zip(source_datasets, dest_datasets):

    # - get group name
    group = os.path.split(dest_path)[0]

    # - copy data
    source.copy(source_path, dest[group])

# ==================================================================================================

# parse command-line options
args = docopt.docopt(__doc__,version='0.0.2')

# check file existence of the destination
if os.path.isfile(args['<destination>']) and not args['--force']:
  quit('File "{0:s}" already exists, continue [y/n]?')

# open HDF5-files
source = h5py.File(args['<source>'     ], 'r')
dest   = h5py.File(args['<destination>'], 'w')

# copy datasets (of which reading is possible)
copydatasets(source, dest, verify(source,getdatasets(source)) )

# close HDF5-files
source.close()
dest  .close()
